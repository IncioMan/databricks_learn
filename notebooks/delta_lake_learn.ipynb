{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207ca7b7-b942-48ca-bffc-9e264ed2cfee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  3|Cathy|\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeDemo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sample DataFrame\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Cathy\")\n",
    "], [\"id\", \"name\"])\n",
    "\n",
    "# Write as Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/data/people\")\n",
    "\n",
    "# Read Delta table\n",
    "df2 = spark.read.format(\"delta\").load(\"/data/people\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3b167-4ec4-4fec-9e1d-3e0a2961a340",
   "metadata": {},
   "source": [
    "# 🔁 1. ACID Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd2f75-6aed-41e0-9e02-0b31a2a9c388",
   "metadata": {
    "tags": []
   },
   "source": [
    "✅ **What Are ACID Transactions?**  \n",
    "ACID stands for:\n",
    "\n",
    "- **Atomicity** → each write is all-or-nothing  \n",
    "- **Consistency** → data moves from one valid state to another  \n",
    "- **Isolation** → concurrent writes don’t interfere  \n",
    "- **Durability** → once a write is committed, it's permanent  \n",
    "\n",
    "Delta Lake brings ACID to data lakes by using a transaction log — just like a database does — even though the data is stored in flat files (Parquet) in object storage or local disk.\n",
    "\n",
    "---\n",
    "\n",
    "🔧 **How Delta Achieves ACID Transactions**  \n",
    "Let’s break it down into mechanisms.\n",
    "\n",
    "### 1. Transaction Log (`_delta_log/`)\n",
    "Every write operation (insert, delete, update, merge, etc.) creates a new log file like:\n",
    "\n",
    "```logs\n",
    "/data/people-delta/_delta_log/00000000000000000000.json\n",
    "/data/people-delta/_delta_log/00000000000000000001.json\n",
    "```\n",
    "\n",
    "\n",
    "Each log file contains:\n",
    "- **Metadata** (`metaData`)  \n",
    "- **Operation info** (`commitInfo`)  \n",
    "- **A list of files added or removed** (`add`, `remove` entries)  \n",
    "\n",
    "🧠 The data itself is immutable Parquet files. Delta just manages which files are “active” by listing them in the latest log version.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Atomicity\n",
    "When you write data:\n",
    "\n",
    "- Delta writes the new Parquet files first.  \n",
    "- Only if *all* new files are successfully written, it appends a JSON log file with `add` entries.  \n",
    "- If anything fails, nothing gets committed to the `_delta_log`, and the table state remains unchanged.  \n",
    "\n",
    "This is atomic: **either the whole change is visible, or none of it is.**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Isolation via File Locking\n",
    "Delta uses concurrency control:\n",
    "\n",
    "- **Local file systems:** it can use file locks  \n",
    "- **Cloud stores like S3:** it uses a protocol called *Optimistic Concurrency Control (OCC)*  \n",
    "\n",
    "With OCC:\n",
    "- A transaction reads the current state (e.g., version 12)  \n",
    "- It prepares new changes based on that state  \n",
    "- When it commits, it checks whether version 12 is still the latest  \n",
    "- If someone else has committed version 13 in the meantime → it fails and retries  \n",
    "\n",
    "This keeps multiple concurrent writers isolated and avoids conflicts.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Durability\n",
    "The `_delta_log` is an append-only log, and once written:\n",
    "\n",
    "- It is **never modified**  \n",
    "- It is considered the **source of truth**  \n",
    "- Even if the cluster crashes, you can rebuild the table’s current state from these logs  \n",
    "\n",
    "That’s your **durability guarantee**.\n",
    "\n",
    "---\n",
    "\n",
    "⚠️ **Does It Affect Performance?**  \n",
    "Yes, but in a good way (mostly)!\n",
    "\n",
    "✅ **Benefits**\n",
    "- Querying is faster: because the log tells you exactly which files to read, instead of scanning the whole folder  \n",
    "- Appends and updates are efficient and safe  \n",
    "\n",
    "⚠️ **Trade-offs**\n",
    "- Small writes result in many small Parquet files → can affect read performance  \n",
    "- That’s why Delta has **Optimize** and **Z-Ordering** to compact and organize files  \n",
    "- For large-scale use, write performance is *slightly* slower than blind dumping to Parquet — but you gain **huge advantages** in reliability, auditability, and maintainability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7d7eb4e-502a-4bd2-8a3c-9f5218be6c37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000000.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": false,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "4",
         "numOutputBytes": "2522",
         "numOutputRows": "3"
        },
        "operationParameters": {
         "mode": "Overwrite",
         "partitionBy": "[]"
        },
        "timestamp": 1745399994525,
        "txnId": "5524117a-1ddb-4c22-9145-9825949f65ed"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "protocol": {
        "minReaderVersion": 1,
        "minWriterVersion": 2
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "metaData": {
        "configuration": {},
        "createdTime": 1745399994116,
        "format": {
         "options": {},
         "provider": "parquet"
        },
        "id": "81c269bc-aa0c-4607-ba98-e1bedf7cbbc6",
        "partitionColumns": [],
        "schemaString": "{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399994411,
        "partitionValues": {},
        "path": "part-00003-567c0df7-46a4-40ad-8c72-df42d46acc0b-c000.snappy.parquet",
        "size": 719,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":1,\"name\":\"Alice\"},\"maxValues\":{\"id\":1,\"name\":\"Alice\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399994426,
        "partitionValues": {},
        "path": "part-00007-6f389545-6fd8-48c0-8dcb-8ad922c1955c-c000.snappy.parquet",
        "size": 705,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":2,\"name\":\"Bob\"},\"maxValues\":{\"id\":2,\"name\":\"Bob\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399994422,
        "partitionValues": {},
        "path": "part-00011-d731241f-f722-408e-839e-22e519881bad-c000.snappy.parquet",
        "size": 719,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":3,\"name\":\"Cathy\"},\"maxValues\":{\"id\":3,\"name\":\"Cathy\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000001.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "1091",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 0,
        "timestamp": 1745399996132,
        "txnId": "ea5b6776-57e9-4aee-8fbe-2486a4208079"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399996108,
        "partitionValues": {},
        "path": "part-00011-117cd441-0a9e-4987-adfa-f23306713746-c000.snappy.parquet",
        "size": 712,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":1,\"name\":\"test\"},\"maxValues\":{\"id\":1,\"name\":\"test\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shutil\n",
    "import json\n",
    "\n",
    "# Path to your Delta table\n",
    "path = \"/data/test-acid\"\n",
    "\n",
    "# Delete the entire folder\n",
    "shutil.rmtree(path, ignore_errors=True)\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Cathy\")\n",
    "], [\"id\", \"name\"])\n",
    "\n",
    "# Write some data\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/data/test-acid\")\n",
    "\n",
    "# Append new row\n",
    "spark.createDataFrame([(4, \"test\")], [\"id\", \"name\"]) \\\n",
    "    .write.format(\"delta\").mode(\"append\").save(\"/data/test-acid\")\n",
    "\n",
    "from IPython.display import display, JSON\n",
    "\n",
    "log_dir = \"/data/test-acid/_delta_log\"\n",
    "log_files = sorted([f for f in os.listdir(log_dir) if f.endswith(\".json\")])\n",
    "\n",
    "for file in log_files:\n",
    "    print(f\"\\n>>> Log version: {file}\")\n",
    "    with open(os.path.join(log_dir, file)) as f:\n",
    "        for line in f.readlines():\n",
    "            parsed = json.loads(line)\n",
    "            display(JSON(parsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f936e9f-7ad1-4da7-a9c0-868745410b1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## _delta_logs Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653083c9-8ac0-4774-96dc-20bb6a9eb01b",
   "metadata": {},
   "source": [
    "### 🧾 `commitInfo` Example Breakdown\n",
    "\n",
    "This is the metadata for the commit.\n",
    "\n",
    "- **`timestamp`**:  \n",
    "  The time when the operation was committed (in milliseconds since Unix epoch).\n",
    "\n",
    "- **`operation`**:  \n",
    "  This log entry represents a `WRITE` operation (it could also be `INSERT`, `UPDATE`, `DELETE`, etc.).  \n",
    "  Here, it’s a `WRITE` operation.\n",
    "\n",
    "- **`operationParameters`**:\n",
    "  - **`mode`**: `Overwrite` — the data is being written to the table, replacing the previous content.  \n",
    "  - **`partitionBy`**: `[]` — indicates the partitioning scheme. An empty list means the table isn’t partitioned.  \n",
    "  - **`isolationLevel`**: `Serializable` — the highest level of isolation (no other operations can interfere during this transaction).  \n",
    "  - **`isBlindAppend`**: `False` — this is not a simple append; it’s a full overwrite.\n",
    "\n",
    "- **`operationMetrics`**:  \n",
    "  Provides stats for the operation:\n",
    "  - **`numFiles`**: 4 new files were written.  \n",
    "  - **`numOutputRows`**: 3 rows of data were written.  \n",
    "  - **`numOutputBytes`**: 2522 bytes — the total size of the written data.\n",
    "\n",
    "- **`txnId`**:  \n",
    "  The transaction ID (unique to the transaction).  \n",
    "  Used to track this specific operation in Delta’s transaction log.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f78bc17-9518-43a9-9394-3fce2f18f027",
   "metadata": {},
   "source": [
    "### 📦 `metaData` Example Breakdown\n",
    "\n",
    "Contains schema and metadata information for the Delta table.\n",
    "\n",
    "- **`id`**:  \n",
    "  A unique ID for the Delta table (used internally for reference).\n",
    "\n",
    "- **`format`**:  \n",
    "  The format used for storing data.  \n",
    "  Here, it's `parquet`, the underlying storage format for Delta.\n",
    "\n",
    "- **`schemaString`**:  \n",
    "  The schema of the table, defined as a JSON string. This tells you the structure of the data (columns, types, etc.).  \n",
    "  In this case:\n",
    "  - `id`: A `long` field (nullable)  \n",
    "  - `name`: A `string` field (nullable)\n",
    "\n",
    "- **`partitionColumns`**:  \n",
    "  The table isn’t partitioned (`[]`).\n",
    "\n",
    "- **`configuration`**:  \n",
    "  Additional configuration (empty here).\n",
    "\n",
    "- **`createdTime`**:  \n",
    "  The time the table was created (in milliseconds since Unix epoch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590804e6-7934-4f8e-ae9a-54fb62d17619",
   "metadata": {},
   "source": [
    "### ➕ `add` Entry Breakdown\n",
    "\n",
    "This entry indicates that a new Parquet file was added to the Delta table.\n",
    "\n",
    "- **`path`**:  \n",
    "  The path to the newly added Parquet file.\n",
    "\n",
    "- **`partitionValues`**:  \n",
    "  Empty here since the table isn’t partitioned.\n",
    "\n",
    "- **`size`**:  \n",
    "  The size of the Parquet file in bytes (`719` bytes).\n",
    "\n",
    "- **`modificationTime`**:  \n",
    "  The timestamp when the file was last modified.\n",
    "\n",
    "- **`dataChange`**:  \n",
    "  `True` means that this file contains **data changes** (not just metadata).\n",
    "\n",
    "- **`stats`**:  \n",
    "  The statistics for the data in this file:\n",
    "  - **`numRecords`**: This file contains 1 record.  \n",
    "  - **`minValues`** and **`maxValues`**:  \n",
    "    The minimum and maximum values of the fields in the file.  \n",
    "    Helps optimize queries (e.g., for pruning files based on min/max).  \n",
    "  - **`nullCount`**:  \n",
    "    The number of null values in each field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eab660-97a3-4725-be08-d037254a6b48",
   "metadata": {},
   "source": [
    "# ⏳ Time Travel in Delta Lake\n",
    "\n",
    "Time Travel in Delta Lake allows you to query historical versions of your data, even after updates, deletes, or appends. This feature is possible thanks to Delta Lake’s transaction log.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 How Time Travel Works\n",
    "\n",
    "Delta Lake uses the `_delta_log` to track all changes to the table. Each write operation (INSERT, UPDATE, DELETE, etc.) appends a new JSON file to the log, containing metadata about the operation.\n",
    "\n",
    "#### 🗂 Transaction Log\n",
    "Each commit is stored as a new JSON file in `_delta_log/`, including:\n",
    "- The operation type (`WRITE`, `UPDATE`, `DELETE`)\n",
    "- The schema at that point\n",
    "- Data files written or removed\n",
    "- The resulting table version\n",
    "\n",
    "#### 🔢 Versions\n",
    "Each commit increments the table version:\n",
    "- Version 0 is the initial state\n",
    "- Version 1 is the next, and so on\n",
    "- Each commit includes a unique `txnId`\n",
    "\n",
    "#### 🧾 Snapshotting\n",
    "When querying, Delta reads the latest log version and constructs a snapshot.  \n",
    "To read an older version, it uses the corresponding log version and rebuilds the table as it was.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Mechanics Behind Time Travel\n",
    "\n",
    "- **Immutable Logs**: Once written, logs are never changed.\n",
    "- **Immutable Data**: Parquet files are also immutable.\n",
    "- **Versioning**: Delta tracks which files belong to each version.\n",
    "- **Garbage Collection**: Old versions are cleaned up via `VACUUM` after a retention period.\n",
    "\n",
    "---\n",
    "\n",
    "### 📅 How to Query Historical Versions\n",
    "\n",
    "#### By Version Number\n",
    "```python\n",
    "df = spark.read.format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 0) \\\n",
    "    .load(\"path_to_table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7b62d-eafc-4531-962f-972cccd75310",
   "metadata": {},
   "source": [
    "#### 🕰️ By Timestamp\n",
    "Delta finds the most recent version that was committed **before** the specified timestamp and returns the snapshot of that version.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Performance Considerations of Time Travel\n",
    "\n",
    "#### 🔄 **Read Performance**:\n",
    "Time travel operations are efficient because Delta doesn’t re-read all data. Instead, it:\n",
    "- Uses the transaction log and metadata to only read necessary files.\n",
    "- Might need to read multiple files, but avoids scanning the entire table.\n",
    "- Leverages Delta’s query optimizer, which efficiently prunes data based on the log's statistics.\n",
    "\n",
    "#### 🗑️ **Garbage Collection**:\n",
    "- Time travel doesn't indefinitely increase storage size.\n",
    "- Delta performs garbage collection to remove old versions when they are no longer needed.\n",
    "- This process is controlled by the retention period and can be manually triggered using the `VACUUM` command to delete old data files no longer referenced.\n",
    "\n",
    "#### 💾 **Storage Overhead**:\n",
    "- There is some overhead due to new Parquet files and logs being created with each version.\n",
    "- However, this is minimized by Delta’s efficient handling of small incremental writes.\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 Example Use Case\n",
    "\n",
    "Suppose you want to go back to a version of the Delta table as it was on **2025-04-20 at 15:30**. Here’s how you would proceed:\n",
    "\n",
    "1. **Check Table History** to find the closest version:\n",
    "```python\n",
    "delta_table = DeltaTable.forPath(spark, \"path_to_table\")\n",
    "history_df = delta_table.history()\n",
    "history_df.show(truncate=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0bee0f1-17d4-4c53-ba06-283a35444e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|1      |2025-04-23 09:19:56.172|null  |null    |WRITE    |{mode -> Append, partitionBy -> []}   |null|null    |null     |0          |Serializable  |true         |{numFiles -> 2, numOutputRows -> 1, numOutputBytes -> 1091}|null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "|0      |2025-04-23 09:19:54.588|null  |null    |WRITE    |{mode -> Overwrite, partitionBy -> []}|null|null    |null     |null       |Serializable  |false        |{numFiles -> 4, numOutputRows -> 3, numOutputBytes -> 2522}|null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"/data/test-acid\")\n",
    "history_df = delta_table.history()\n",
    "history_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a771109-e399-4faa-9448-e684367502f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_table.delete(\"id = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b255363-e631-4683-a70b-3d4f5aa0fce6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                           |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|2      |2025-04-23 09:33:20.121|null  |null    |DELETE   |{predicate -> [\"(id = 1L)\"]}          |null|null    |null     |1          |Serializable  |false        |{numRemovedFiles -> 2, numRemovedBytes -> 1431, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 964, numDeletedRows -> 2, scanTimeMs -> 654, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 310}|null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "|1      |2025-04-23 09:19:56.172|null  |null    |WRITE    |{mode -> Append, partitionBy -> []}   |null|null    |null     |0          |Serializable  |true         |{numFiles -> 2, numOutputRows -> 1, numOutputBytes -> 1091}                                                                                                                                                                |null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "|0      |2025-04-23 09:19:54.588|null  |null    |WRITE    |{mode -> Overwrite, partitionBy -> []}|null|null    |null     |null       |Serializable  |false        |{numFiles -> 4, numOutputRows -> 3, numOutputBytes -> 2522}                                                                                                                                                                |null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_df = delta_table.history()\n",
    "history_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c038e626-dc6f-4b08-a2a2-539b7f7e254e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  1| test|\n",
      "+---+-----+\n",
      "\n",
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/data/test-acid\").filter(\"id = 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b0cb866-d670-4579-91b5-a13b7d81246f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"/data/test-acid\").filter(\"id = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c36ac3f-c3be-4a4b-9255-b5f0fbf9c8e3",
   "metadata": {},
   "source": [
    "How Deletes Are Handled in Delta Lake\n",
    "Delta Lake is append-only under the hood. It never modifies existing data files directly. Instead, it follows a copy-on-write strategy:\n",
    "\n",
    "1. Query Optimization and File Pruning\n",
    "When you run a DELETE or an UPDATE, Delta first identifies which Parquet files contain the rows to be deleted using the statistics in the _delta_log (like minValues, maxValues, and nullCounts).\n",
    "\n",
    "Only those files are read and rewritten. Others are untouched.\n",
    "\n",
    "2. Rewrite with Changes\n",
    "Delta reads the affected Parquet files, applies the delete condition (e.g., WHERE id = 42), and writes new files with the remaining (non-deleted) records.\n",
    "\n",
    "The old Parquet files are logically removed — they are not physically deleted immediately but are marked as removed in the _delta_log.\n",
    "\n",
    "3. Transaction Log Update\n",
    "The _delta_log records:\n",
    "\n",
    "A new version with a commitInfo block describing the DELETE operation.\n",
    "\n",
    "A set of remove actions, each pointing to a deleted (now obsolete) data file.\n",
    "\n",
    "A set of add actions for the newly written files.\n",
    "\n",
    "So from a transactional point of view:\n",
    "\n",
    "The table instantly reflects the updated state (with the records deleted).\n",
    "\n",
    "Old files still exist and can be used to reconstruct previous versions — which is what enables time travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04f8ee1b-2a8b-406b-8032-ec5787ff2b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000000.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": false,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "4",
         "numOutputBytes": "2522",
         "numOutputRows": "3"
        },
        "operationParameters": {
         "mode": "Overwrite",
         "partitionBy": "[]"
        },
        "timestamp": 1745399994525,
        "txnId": "5524117a-1ddb-4c22-9145-9825949f65ed"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "protocol": {
        "minReaderVersion": 1,
        "minWriterVersion": 2
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "metaData": {
        "configuration": {},
        "createdTime": 1745399994116,
        "format": {
         "options": {},
         "provider": "parquet"
        },
        "id": "81c269bc-aa0c-4607-ba98-e1bedf7cbbc6",
        "partitionColumns": [],
        "schemaString": "{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399994411,
        "partitionValues": {},
        "path": "part-00003-567c0df7-46a4-40ad-8c72-df42d46acc0b-c000.snappy.parquet",
        "size": 719,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":1,\"name\":\"Alice\"},\"maxValues\":{\"id\":1,\"name\":\"Alice\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399994426,
        "partitionValues": {},
        "path": "part-00007-6f389545-6fd8-48c0-8dcb-8ad922c1955c-c000.snappy.parquet",
        "size": 705,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":2,\"name\":\"Bob\"},\"maxValues\":{\"id\":2,\"name\":\"Bob\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399994422,
        "partitionValues": {},
        "path": "part-00011-d731241f-f722-408e-839e-22e519881bad-c000.snappy.parquet",
        "size": 719,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":3,\"name\":\"Cathy\"},\"maxValues\":{\"id\":3,\"name\":\"Cathy\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000001.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "1091",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 0,
        "timestamp": 1745399996132,
        "txnId": "ea5b6776-57e9-4aee-8fbe-2486a4208079"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399996108,
        "partitionValues": {},
        "path": "part-00011-117cd441-0a9e-4987-adfa-f23306713746-c000.snappy.parquet",
        "size": 712,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":1,\"name\":\"test\"},\"maxValues\":{\"id\":1,\"name\":\"test\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000002.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": false,
        "isolationLevel": "Serializable",
        "operation": "DELETE",
        "operationMetrics": {
         "executionTimeMs": "964",
         "numAddedBytes": "0",
         "numAddedChangeFiles": "0",
         "numAddedFiles": "0",
         "numCopiedRows": "0",
         "numDeletedRows": "2",
         "numRemovedBytes": "1431",
         "numRemovedFiles": "2",
         "rewriteTimeMs": "310",
         "scanTimeMs": "654"
        },
        "operationParameters": {
         "predicate": "[\"(id = 1L)\"]"
        },
        "readVersion": 1,
        "timestamp": 1745400800049,
        "txnId": "2984792e-f799-44db-a9f2-2238fb5c88b1"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "remove": {
        "dataChange": true,
        "deletionTimestamp": 1745400800049,
        "extendedFileMetadata": true,
        "partitionValues": {},
        "path": "part-00003-567c0df7-46a4-40ad-8c72-df42d46acc0b-c000.snappy.parquet",
        "size": 719
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "remove": {
        "dataChange": true,
        "deletionTimestamp": 1745400800049,
        "extendedFileMetadata": true,
        "partitionValues": {},
        "path": "part-00011-117cd441-0a9e-4987-adfa-f23306713746-c000.snappy.parquet",
        "size": 712
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, JSON\n",
    "\n",
    "log_dir = \"/data/test-acid/_delta_log\"\n",
    "log_files = sorted([f for f in os.listdir(log_dir) if f.endswith(\".json\")])\n",
    "\n",
    "for file in log_files:\n",
    "    print(f\"\\n>>> Log version: {file}\")\n",
    "    with open(os.path.join(log_dir, file)) as f:\n",
    "        for line in f.readlines():\n",
    "            parsed = json.loads(line)\n",
    "            display(JSON(parsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e01ea-d93b-4777-9633-eef7fb0d9b27",
   "metadata": {},
   "source": [
    "🕰️ Why Time Travel Leads to Storage Growth\n",
    "Every Delta operation (write, update, delete) doesn't modify files in-place — it writes new Parquet files and logs a new version in _delta_log.\n",
    "\n",
    "So over time:\n",
    "\n",
    "More versions accumulate in _delta_log/\n",
    "\n",
    "More old Parquet files (not referenced by the latest table state) accumulate\n",
    "\n",
    "This can lead to:\n",
    "\n",
    "Higher storage costs\n",
    "\n",
    "Slightly slower read performance, especially for metadata-heavy operations like full scans or schema inference, if you don’t manage it\n",
    "\n",
    "🔍 How This Affects Performance\n",
    "Reading the latest snapshot is fast, because Delta uses a transaction log and checkpoints to resolve it quickly.\n",
    "\n",
    "But:\n",
    "\n",
    "The more logs there are, the longer it takes to process them during startup (if no recent checkpoint exists).\n",
    "\n",
    "If you use versionAsOf or timestampAsOf, Delta might have to scan logs backwards and reconstruct the state from previous versions.\n",
    "\n",
    "Over time, many small files can also lead to file listing overhead, especially on cloud storage.\n",
    "\n",
    "🧹 How to Manage This (Best Practices)\n",
    "Here’s how Delta Lake helps you keep things clean and fast:\n",
    "\n",
    "✅ 1. VACUUM – Physical Cleanup\n",
    "Delta retains old files for a default of 7 days, even if they’re not used by the current table version. This is to support time travel.\n",
    "\n",
    "You can manually or automatically remove obsolete files:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from delta.tables import DeltaTable\n",
    "delta_table = DeltaTable.forPath(spark, \"path_to_table\")\n",
    "delta_table.vacuum(retentionHours=168)  # 7 days = 168 hours\n",
    "Tip: You can set a shorter retention (e.g. 1 hour) only if you're sure you don’t need time travel to those states:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "delta_table.vacuum(retentionHours=1)  # Dangerous if others rely on old versions\n",
    "✅ 2. CHECKPOINTING – Faster Metadata Resolution\n",
    "Delta Lake creates checkpoint files every N versions (default: 10). These are compact versions of the state up to that point. That way, Delta can rebuild the current snapshot quickly without replaying the entire log history.\n",
    "\n",
    "You can force a checkpoint manually:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "delta_table = DeltaTable.forPath(spark, \"path_to_table\")\n",
    "spark._jsparkSession.sessionState().catalog().asTableCatalog().loadTableMetadata(\n",
    "    spark._jsparkSession.sessionState().sqlParser().parseTableIdentifier(\"delta.`path_to_table`\")\n",
    ")\n",
    "(That’s the internal way — but usually you just let Delta manage it.)\n",
    "\n",
    "✅ 3. LOG CLEANUP – Transaction Log Compaction\n",
    "Delta also retains all those .json log files (one per version). These are small, but over time many can add up.\n",
    "\n",
    "If you're using a cloud storage bucket, having 100k log files can slow down file listing operations. Delta handles this by:\n",
    "\n",
    "Using checkpoints\n",
    "\n",
    "Compaction of logs\n",
    "\n",
    "You can also manually remove old logs if you’ve vacuumed and don’t need time travel to those versions anymore.\n",
    "\n",
    "✅ 4. Tuning Retention Policies\n",
    "You can configure these defaults globally or per table:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")  # Needed for <7d\n",
    "Or set table properties:\n",
    "\n",
    "sql\n",
    "Copy\n",
    "Edit\n",
    "ALTER TABLE delta.`path_to_table` SET TBLPROPERTIES ('delta.deletedFileRetentionDuratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5866e6b3-49ef-488d-85c3-9cb4dbb3ea75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"DeltaCheckpointVacuumDemo\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1\")  # to reduce files for testing\n",
    "\n",
    "spark = builder.getOrCreate()\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f978363f-ddb7-40e1-a30e-0cbef1579473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "base_path = \"/data/delta/checkpoint_test\"\n",
    "shutil.rmtree(base_path, ignore_errors=True)  # Clean slate\n",
    "\n",
    "df = spark.range(1).withColumnRenamed(\"id\", \"val\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a71642d5-e264-48bc-97d6-6466c5f3f0e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".part-00000-270402e2-5b13-440f-b7b8-39475c1e2dc9-c000.snappy.parquet.crc\n",
      ".part-00011-a9937862-201c-4282-b1cc-db8efa759d4d-c000.snappy.parquet.crc\n",
      "_delta_log\n",
      "part-00000-270402e2-5b13-440f-b7b8-39475c1e2dc9-c000.snappy.parquet\n",
      "part-00011-a9937862-201c-4282-b1cc-db8efa759d4d-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Inspect _delta_log/\n",
    "import os\n",
    "\n",
    "log_files = sorted(os.listdir(f\"{base_path}/\"))\n",
    "for f in log_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36bbd59f-cd94-43a8-b3e1-c2673ab6724a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended version 1\n",
      "Appended version 2\n",
      "Appended version 3\n",
      "Appended version 4\n",
      "Appended version 5\n",
      "Appended version 6\n",
      "Appended version 7\n",
      "Appended version 8\n",
      "Appended version 9\n",
      "Appended version 10\n",
      "Appended version 11\n"
     ]
    }
   ],
   "source": [
    "#This will generate 11 commit files and automatically trigger a checkpoint at version 10.\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "dt = DeltaTable.forPath(spark, base_path)\n",
    "\n",
    "for i in range(1, 12):\n",
    "    df = spark.range(i, i + 1).withColumnRenamed(\"id\", \"val\")\n",
    "    df.write.format(\"delta\").mode(\"append\").save(base_path)\n",
    "    print(f\"Appended version {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6227a22-58bc-458a-87d8-a05959279ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".part-00000-11da9a11-3f34-4247-a787-89fac4f509cd-c000.snappy.parquet.crc\n",
      ".part-00000-270402e2-5b13-440f-b7b8-39475c1e2dc9-c000.snappy.parquet.crc\n",
      ".part-00000-4b81e43d-fd73-4740-95ea-0afe6901dabe-c000.snappy.parquet.crc\n",
      ".part-00000-6d68a7ee-59b9-4e62-8c6b-d8c51ba2d979-c000.snappy.parquet.crc\n",
      ".part-00000-8ce5a978-e36d-42af-8757-9ba38e0ad4be-c000.snappy.parquet.crc\n",
      ".part-00000-9758411a-2580-4b2b-8ccc-1fe73d4a7fe7-c000.snappy.parquet.crc\n",
      ".part-00000-caa341f1-b64e-48b3-95d2-ea09a13de127-c000.snappy.parquet.crc\n",
      ".part-00000-d3814381-4e98-4e82-b4c6-e72b84eb2f06-c000.snappy.parquet.crc\n",
      ".part-00000-ece421c8-b395-41be-b4ca-84b9ddbd9217-c000.snappy.parquet.crc\n",
      ".part-00000-f36285f7-de4b-4278-b8b7-096475dfa988-c000.snappy.parquet.crc\n",
      ".part-00000-f86c7aac-016a-487d-a3ef-0a67dbf29566-c000.snappy.parquet.crc\n",
      ".part-00000-ff4b16d4-7ee8-4bcc-a07e-00b5f5942423-c000.snappy.parquet.crc\n",
      ".part-00011-09c33597-309a-4a9d-b242-cc54eab5a897-c000.snappy.parquet.crc\n",
      ".part-00011-211c2702-6c60-4d7f-8c64-536818841fa3-c000.snappy.parquet.crc\n",
      ".part-00011-43f2c617-ed91-437f-8323-0ca3e695d631-c000.snappy.parquet.crc\n",
      ".part-00011-4a7d0559-4db7-4325-a70d-1e7dc75b3aae-c000.snappy.parquet.crc\n",
      ".part-00011-58a54a3d-e88c-4b5b-b653-443493b0ad8f-c000.snappy.parquet.crc\n",
      ".part-00011-65fe9fba-8933-43f1-bb5e-84fb09266348-c000.snappy.parquet.crc\n",
      ".part-00011-80edde40-2762-490c-a8af-d2e116f06862-c000.snappy.parquet.crc\n",
      ".part-00011-85bf4d49-db62-42a0-867e-6a8c15d26996-c000.snappy.parquet.crc\n",
      ".part-00011-9aab9cdd-80ff-43cd-be28-9ff79af7b0d4-c000.snappy.parquet.crc\n",
      ".part-00011-a9937862-201c-4282-b1cc-db8efa759d4d-c000.snappy.parquet.crc\n",
      ".part-00011-aed8a53c-0a16-454f-8129-b8fc184730e9-c000.snappy.parquet.crc\n",
      ".part-00011-dbaa4880-f643-4cd8-97e5-dad611594348-c000.snappy.parquet.crc\n",
      "_delta_log\n",
      "part-00000-11da9a11-3f34-4247-a787-89fac4f509cd-c000.snappy.parquet\n",
      "part-00000-270402e2-5b13-440f-b7b8-39475c1e2dc9-c000.snappy.parquet\n",
      "part-00000-4b81e43d-fd73-4740-95ea-0afe6901dabe-c000.snappy.parquet\n",
      "part-00000-6d68a7ee-59b9-4e62-8c6b-d8c51ba2d979-c000.snappy.parquet\n",
      "part-00000-8ce5a978-e36d-42af-8757-9ba38e0ad4be-c000.snappy.parquet\n",
      "part-00000-9758411a-2580-4b2b-8ccc-1fe73d4a7fe7-c000.snappy.parquet\n",
      "part-00000-caa341f1-b64e-48b3-95d2-ea09a13de127-c000.snappy.parquet\n",
      "part-00000-d3814381-4e98-4e82-b4c6-e72b84eb2f06-c000.snappy.parquet\n",
      "part-00000-ece421c8-b395-41be-b4ca-84b9ddbd9217-c000.snappy.parquet\n",
      "part-00000-f36285f7-de4b-4278-b8b7-096475dfa988-c000.snappy.parquet\n",
      "part-00000-f86c7aac-016a-487d-a3ef-0a67dbf29566-c000.snappy.parquet\n",
      "part-00000-ff4b16d4-7ee8-4bcc-a07e-00b5f5942423-c000.snappy.parquet\n",
      "part-00011-09c33597-309a-4a9d-b242-cc54eab5a897-c000.snappy.parquet\n",
      "part-00011-211c2702-6c60-4d7f-8c64-536818841fa3-c000.snappy.parquet\n",
      "part-00011-43f2c617-ed91-437f-8323-0ca3e695d631-c000.snappy.parquet\n",
      "part-00011-4a7d0559-4db7-4325-a70d-1e7dc75b3aae-c000.snappy.parquet\n",
      "part-00011-58a54a3d-e88c-4b5b-b653-443493b0ad8f-c000.snappy.parquet\n",
      "part-00011-65fe9fba-8933-43f1-bb5e-84fb09266348-c000.snappy.parquet\n",
      "part-00011-80edde40-2762-490c-a8af-d2e116f06862-c000.snappy.parquet\n",
      "part-00011-85bf4d49-db62-42a0-867e-6a8c15d26996-c000.snappy.parquet\n",
      "part-00011-9aab9cdd-80ff-43cd-be28-9ff79af7b0d4-c000.snappy.parquet\n",
      "part-00011-a9937862-201c-4282-b1cc-db8efa759d4d-c000.snappy.parquet\n",
      "part-00011-aed8a53c-0a16-454f-8129-b8fc184730e9-c000.snappy.parquet\n",
      "part-00011-dbaa4880-f643-4cd8-97e5-dad611594348-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Inspect parquet files\n",
    "import os\n",
    "\n",
    "log_files = sorted(os.listdir(f\"{base_path}/\"))\n",
    "for f in log_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b6d7cf7-5edf-4ebf-b5ba-f291c20afee3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".00000000000000000000.json.crc\n",
      ".00000000000000000001.json.crc\n",
      ".00000000000000000002.json.crc\n",
      ".00000000000000000003.json.crc\n",
      ".00000000000000000004.json.crc\n",
      ".00000000000000000005.json.crc\n",
      ".00000000000000000006.json.crc\n",
      ".00000000000000000007.json.crc\n",
      ".00000000000000000008.json.crc\n",
      ".00000000000000000009.json.crc\n",
      ".00000000000000000010.checkpoint.parquet.crc\n",
      ".00000000000000000010.json.crc\n",
      ".00000000000000000011.json.crc\n",
      "._last_checkpoint.crc\n",
      "00000000000000000000.json\n",
      "00000000000000000001.json\n",
      "00000000000000000002.json\n",
      "00000000000000000003.json\n",
      "00000000000000000004.json\n",
      "00000000000000000005.json\n",
      "00000000000000000006.json\n",
      "00000000000000000007.json\n",
      "00000000000000000008.json\n",
      "00000000000000000009.json\n",
      "00000000000000000010.checkpoint.parquet\n",
      "00000000000000000010.json\n",
      "00000000000000000011.json\n",
      "_last_checkpoint\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Inspect _delta_log/\n",
    "import os\n",
    "\n",
    "log_files = sorted(os.listdir(f\"{base_path}/_delta_log\"))\n",
    "for f in log_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72c772-f0b4-4e43-9726-a98ba2de9b99",
   "metadata": {},
   "source": [
    "➡️ You should now see:\n",
    "\n",
    "00000000000000000000.json through 00000000000000000010.json\n",
    "\n",
    "A file like 00000000000000000010.checkpoint.parquet\n",
    "This is the checkpoint file: a compacted representation of the table state up to version 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b810d219-7d71-47f9-90ba-cf7a1c81f1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|   operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|     13|2025-04-23 13:39:...|  null|    null|  VACUUM END|{status -> COMPLE...|null|    null|     null|         12|SnapshotIsolation|         true|{numDeletedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|     12|2025-04-23 13:39:...|  null|    null|VACUUM START|{retentionCheckEn...|null|    null|     null|         11|SnapshotIsolation|         true|{numFilesToDelete...|        null|Apache-Spark/3.3....|\n",
      "|     11|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|         10|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|     10|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          9|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      9|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          8|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      8|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          7|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      7|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          6|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      6|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          5|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      5|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          4|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      4|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          3|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      3|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          2|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      2|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          1|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      1|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          0|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      0|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Overwrit...|null|    null|     null|       null|     Serializable|        false|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c641c39-45b9-40f5-a272-95ceb544d9e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#🧹 Step 4: Run VACUUM\n",
    "dt.vacuum(retentionHours=0)  # Only for testing — don't do in production!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b814f8-1d9c-4c7b-b3a8-57dee49e0f6f",
   "metadata": {},
   "source": [
    "The deltaTable.history() function reads from the JSON commit files in _delta_log/, which are not deleted by VACUUM. So you can still trace all the changes made to the table\n",
    "While the history is intact, the ability to time travel to older versions may be affected depending on what VACUUM removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6452278-ae93-461c-8c0c-f0d6958ecf56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|   operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|     13|2025-04-23 13:39:...|  null|    null|  VACUUM END|{status -> COMPLE...|null|    null|     null|         12|SnapshotIsolation|         true|{numDeletedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|     12|2025-04-23 13:39:...|  null|    null|VACUUM START|{retentionCheckEn...|null|    null|     null|         11|SnapshotIsolation|         true|{numFilesToDelete...|        null|Apache-Spark/3.3....|\n",
      "|     11|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|         10|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|     10|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          9|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      9|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          8|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      8|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          7|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      7|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          6|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      6|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          5|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      5|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          4|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      4|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          3|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      3|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          2|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      2|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          1|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      1|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Append, ...|null|    null|     null|          0|     Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "|      0|2025-04-23 13:39:...|  null|    null|       WRITE|{mode -> Overwrit...|null|    null|     null|       null|     Serializable|        false|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6443681a-9bff-4203-bdbd-d6fb36d32a92",
   "metadata": {},
   "source": [
    "You'll likely still see:\n",
    "\n",
    "JSON logs\n",
    "\n",
    "Checkpoint file\n",
    "\n",
    "But old Parquet data files (no longer referenced) will be gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69193fb9-7a17-4474-956d-d8bed2b49051",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".part-00011-04669adb-8cfc-405c-83de-0f2511b61806-c000.snappy.parquet.crc\n",
      ".part-00011-149528b6-5498-4835-bc1b-361df3dc445a-c000.snappy.parquet.crc\n",
      ".part-00011-3b068554-c092-42b1-8836-2c7edfc0e0a1-c000.snappy.parquet.crc\n",
      ".part-00011-48f0ee4e-2497-4e68-a5a2-ae4d252d309e-c000.snappy.parquet.crc\n",
      ".part-00011-4d41da4f-fef5-4979-83ee-d13fe85f930e-c000.snappy.parquet.crc\n",
      ".part-00011-54702229-d38e-436c-a2ab-4c7ea650f713-c000.snappy.parquet.crc\n",
      ".part-00011-630df688-8db7-4995-9734-b989b4c3ab7e-c000.snappy.parquet.crc\n",
      ".part-00011-643eacbb-92e3-432d-bb22-141b6dd3968a-c000.snappy.parquet.crc\n",
      ".part-00011-a114ee84-5f01-4781-8e25-dbea719e3ee0-c000.snappy.parquet.crc\n",
      ".part-00011-cbb048bb-06a4-46b6-b556-92359ee16c0f-c000.snappy.parquet.crc\n",
      ".part-00011-d2d8df03-371e-44e0-b8f2-c858831204a7-c000.snappy.parquet.crc\n",
      ".part-00011-e70089c8-bf8c-478d-81f1-ae6b50d9d18e-c000.snappy.parquet.crc\n",
      "_delta_log\n",
      "part-00011-04669adb-8cfc-405c-83de-0f2511b61806-c000.snappy.parquet\n",
      "part-00011-149528b6-5498-4835-bc1b-361df3dc445a-c000.snappy.parquet\n",
      "part-00011-3b068554-c092-42b1-8836-2c7edfc0e0a1-c000.snappy.parquet\n",
      "part-00011-48f0ee4e-2497-4e68-a5a2-ae4d252d309e-c000.snappy.parquet\n",
      "part-00011-4d41da4f-fef5-4979-83ee-d13fe85f930e-c000.snappy.parquet\n",
      "part-00011-54702229-d38e-436c-a2ab-4c7ea650f713-c000.snappy.parquet\n",
      "part-00011-630df688-8db7-4995-9734-b989b4c3ab7e-c000.snappy.parquet\n",
      "part-00011-643eacbb-92e3-432d-bb22-141b6dd3968a-c000.snappy.parquet\n",
      "part-00011-a114ee84-5f01-4781-8e25-dbea719e3ee0-c000.snappy.parquet\n",
      "part-00011-cbb048bb-06a4-46b6-b556-92359ee16c0f-c000.snappy.parquet\n",
      "part-00011-d2d8df03-371e-44e0-b8f2-c858831204a7-c000.snappy.parquet\n",
      "part-00011-e70089c8-bf8c-478d-81f1-ae6b50d9d18e-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Inspect parquet files\n",
    "import os\n",
    "\n",
    "log_files = sorted(os.listdir(f\"{base_path}/\"))\n",
    "for f in log_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb812006-43f0-4d70-b5ea-e3d0099ca568",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".00000000000000000000.json.crc\n",
      ".00000000000000000001.json.crc\n",
      ".00000000000000000002.json.crc\n",
      ".00000000000000000003.json.crc\n",
      ".00000000000000000004.json.crc\n",
      ".00000000000000000005.json.crc\n",
      ".00000000000000000006.json.crc\n",
      ".00000000000000000007.json.crc\n",
      ".00000000000000000008.json.crc\n",
      ".00000000000000000009.json.crc\n",
      ".00000000000000000010.checkpoint.parquet.crc\n",
      ".00000000000000000010.json.crc\n",
      ".00000000000000000011.json.crc\n",
      ".00000000000000000012.json.crc\n",
      ".00000000000000000013.json.crc\n",
      ".00000000000000000014.json.crc\n",
      ".00000000000000000015.json.crc\n",
      "._last_checkpoint.crc\n",
      "00000000000000000000.json\n",
      "00000000000000000001.json\n",
      "00000000000000000002.json\n",
      "00000000000000000003.json\n",
      "00000000000000000004.json\n",
      "00000000000000000005.json\n",
      "00000000000000000006.json\n",
      "00000000000000000007.json\n",
      "00000000000000000008.json\n",
      "00000000000000000009.json\n",
      "00000000000000000010.checkpoint.parquet\n",
      "00000000000000000010.json\n",
      "00000000000000000011.json\n",
      "00000000000000000012.json\n",
      "00000000000000000013.json\n",
      "00000000000000000014.json\n",
      "00000000000000000015.json\n",
      "_last_checkpoint\n"
     ]
    }
   ],
   "source": [
    "#Then re-inspect the _delta_log/ folder:\n",
    "log_files_after_vacuum = sorted(os.listdir(f\"{base_path}/_delta_log\"))\n",
    "for f in log_files_after_vacuum:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7285d17b-c96a-460a-9509-ffc5c9fc897b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000000.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": false,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "779",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Overwrite",
         "partitionBy": "[]"
        },
        "timestamp": 1745415165846,
        "txnId": "857f20f1-aa29-4446-9ccd-7dc5a2cb9bf0"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "protocol": {
        "minReaderVersion": 1,
        "minWriterVersion": 2
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "metaData": {
        "configuration": {},
        "createdTime": 1745415165624,
        "format": {
         "options": {},
         "provider": "parquet"
        },
        "id": "00de0bd7-c80f-49b4-9b33-9b7c4954c34e",
        "partitionColumns": [],
        "schemaString": "{\"type\":\"struct\",\"fields\":[{\"name\":\"val\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745415165740,
        "partitionValues": {},
        "path": "part-00011-643eacbb-92e3-432d-bb22-141b6dd3968a-c000.snappy.parquet",
        "size": 481,
        "stats": "{\"numRecords\":1,\"minValues\":{\"val\":0},\"maxValues\":{\"val\":0},\"nullCount\":{\"val\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000001.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "779",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 0,
        "timestamp": 1745415188167,
        "txnId": "5ea6df1d-1729-4c25-a531-3d8d299d49fc"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745415188130,
        "partitionValues": {},
        "path": "part-00011-48f0ee4e-2497-4e68-a5a2-ae4d252d309e-c000.snappy.parquet",
        "size": 481,
        "stats": "{\"numRecords\":1,\"minValues\":{\"val\":1},\"maxValues\":{\"val\":1},\"nullCount\":{\"val\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000002.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "779",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 1,
        "timestamp": 1745415189463,
        "txnId": "ac4150e9-0c9c-4d58-b608-5ece121fab12"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745415189392,
        "partitionValues": {},
        "path": "part-00011-149528b6-5498-4835-bc1b-361df3dc445a-c000.snappy.parquet",
        "size": 481,
        "stats": "{\"numRecords\":1,\"minValues\":{\"val\":2},\"maxValues\":{\"val\":2},\"nullCount\":{\"val\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000003.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "779",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 2,
        "timestamp": 1745415190564,
        "txnId": "97228b6d-e93f-4034-863a-49b4b207e42d"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745415190501,
        "partitionValues": {},
        "path": "part-00011-3b068554-c092-42b1-8836-2c7edfc0e0a1-c000.snappy.parquet",
        "size": 481,
        "stats": "{\"numRecords\":1,\"minValues\":{\"val\":3},\"maxValues\":{\"val\":3},\"nullCount\":{\"val\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000004.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "779",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 3,
        "timestamp": 1745415191914,
        "txnId": "30fa1cbf-1f0b-443f-afe3-f490388f544f"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745415191876,
        "partitionValues": {},
        "path": "part-00011-4d41da4f-fef5-4979-83ee-d13fe85f930e-c000.snappy.parquet",
        "size": 481,
        "stats": "{\"numRecords\":1,\"minValues\":{\"val\":4},\"maxValues\":{\"val\":4},\"nullCount\":{\"val\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000005.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "779",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 4,
        "timestamp": 1745415193354,
        "txnId": "edd81044-e3fa-4661-898a-3af5cb9d380e"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745415193326,
        "partitionValues": {},
        "path": "part-00011-54702229-d38e-436c-a2ab-4c7ea650f713-c000.snappy.parquet",
        "size": 481,
        "stats": "{\"numRecords\":1,\"minValues\":{\"val\":5},\"maxValues\":{\"val\":5},\"nullCount\":{\"val\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000006.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "779",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 5,
        "timestamp": 1745415195072,
        "txnId": "0e99d18b-1b49-4f3e-a659-3682355c42bd"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745415195034,
        "partitionValues": {},
        "path": "part-00011-a114ee84-5f01-4781-8e25-dbea719e3ee0-c000.snappy.parquet",
        "size": 481,
        "stats": "{\"numRecords\":1,\"minValues\":{\"val\":6},\"maxValues\":{\"val\":6},\"nullCount\":{\"val\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000007.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "779",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 6,
        "timestamp": 1745415196931,
        "txnId": "865425d4-b4e9-47bd-9123-567bdc12805e"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745415196888,
        "partitionValues": {},
        "path": "part-00011-cbb048bb-06a4-46b6-b556-92359ee16c0f-c000.snappy.parquet",
        "size": 481,
        "stats": "{\"numRecords\":1,\"minValues\":{\"val\":7},\"maxValues\":{\"val\":7},\"nullCount\":{\"val\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000008.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "779",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 7,
        "timestamp": 1745415198647,
        "txnId": "49ba212f-1425-449c-8fbe-52f4d9ba3275"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745415198602,
        "partitionValues": {},
        "path": "part-00011-d2d8df03-371e-44e0-b8f2-c858831204a7-c000.snappy.parquet",
        "size": 481,
        "stats": "{\"numRecords\":1,\"minValues\":{\"val\":8},\"maxValues\":{\"val\":8},\"nullCount\":{\"val\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000009.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "779",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 8,
        "timestamp": 1745415200152,
        "txnId": "44c0025c-18ae-481a-b572-134056f19664"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745415200113,
        "partitionValues": {},
        "path": "part-00011-630df688-8db7-4995-9734-b989b4c3ab7e-c000.snappy.parquet",
        "size": 481,
        "stats": "{\"numRecords\":1,\"minValues\":{\"val\":9},\"maxValues\":{\"val\":9},\"nullCount\":{\"val\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000010.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "778",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 9,
        "timestamp": 1745415201688,
        "txnId": "c976cbd4-1cd8-45dd-8d27-1dd433c39fe7"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745415201636,
        "partitionValues": {},
        "path": "part-00011-e70089c8-bf8c-478d-81f1-ae6b50d9d18e-c000.snappy.parquet",
        "size": 480,
        "stats": "{\"numRecords\":1,\"minValues\":{\"val\":10},\"maxValues\":{\"val\":10},\"nullCount\":{\"val\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000011.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "779",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 10,
        "timestamp": 1745415205316,
        "txnId": "148bde0e-2493-48d6-9bfc-9f55f5dfb673"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745415205278,
        "partitionValues": {},
        "path": "part-00011-04669adb-8cfc-405c-83de-0f2511b61806-c000.snappy.parquet",
        "size": 481,
        "stats": "{\"numRecords\":1,\"minValues\":{\"val\":11},\"maxValues\":{\"val\":11},\"nullCount\":{\"val\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000012.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "SnapshotIsolation",
        "operation": "VACUUM START",
        "operationMetrics": {
         "numFilesToDelete": "12",
         "sizeOfDataToDelete": "3576"
        },
        "operationParameters": {
         "defaultRetentionMillis": 604800000,
         "retentionCheckEnabled": false,
         "specifiedRetentionMillis": 0
        },
        "readVersion": 11,
        "timestamp": 1745415232554,
        "txnId": "5e9c2386-0125-4818-9cb2-f05a40a9e835"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000013.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "SnapshotIsolation",
        "operation": "VACUUM END",
        "operationMetrics": {
         "numDeletedFiles": "12",
         "numVacuumedDirectories": "1"
        },
        "operationParameters": {
         "status": "COMPLETED"
        },
        "readVersion": 12,
        "timestamp": 1745415235753,
        "txnId": "abf7c8ea-c7dc-48f1-b477-3c05c3e7909a"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, JSON\n",
    "import json\n",
    "\n",
    "log_dir = f\"{base_path}/_delta_log\"\n",
    "log_files = sorted([f for f in os.listdir(log_dir) if f.endswith(\".json\")])\n",
    "\n",
    "for file in log_files:\n",
    "    print(f\"\\n>>> Log version: {file}\")\n",
    "    with open(os.path.join(log_dir, file)) as f:\n",
    "        for line in f.readlines():\n",
    "            parsed = json.loads(line)\n",
    "            display(JSON(parsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5636d-6e2e-4285-b492-a109660f257c",
   "metadata": {},
   "source": [
    "🛡 3. Schema Enforcement\n",
    "Try writing data with a wrong schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f869015-43b8-4f3b-ad3d-d0638cfdd981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "right_schema = spark.createDataFrame([\n",
    "    (99999, \"John\")\n",
    "], [\"id\", \"name\"])\n",
    "\n",
    "# This will fail!\n",
    "right_schema.write.format(\"delta\").mode(\"append\").save(\"/data/schema-people-delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0bb20b0-b0bc-42ea-81e0-658db6306822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: 950f499f-9acb-4364-b628-7449e65640b1).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- id: long (nullable = true)\n-- name: string (nullable = true)\n\n\nData schema:\nroot\n-- id: long (nullable = true)\n-- name: string (nullable = true)\n-- oops: string (nullable = true)\n\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m wrong_schema \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([\n\u001b[1;32m      2\u001b[0m     (\u001b[38;5;241m99999\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrong_column_here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m ], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moops\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# This will fail!\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mwrong_schema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/schema-people-delta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: 950f499f-9acb-4364-b628-7449e65640b1).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- id: long (nullable = true)\n-- name: string (nullable = true)\n\n\nData schema:\nroot\n-- id: long (nullable = true)\n-- name: string (nullable = true)\n-- oops: string (nullable = true)\n\n         "
     ]
    }
   ],
   "source": [
    "wrong_schema = spark.createDataFrame([\n",
    "    (99999, \"John\", \"wrong_column_here\")\n",
    "], [\"id\", \"name\", \"oops\"])\n",
    "\n",
    "# This will fail!\n",
    "wrong_schema.write.format(\"delta\").mode(\"append\").save(\"/data/schema-people-delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c6a742-eb56-449a-8933-afc89d5edb59",
   "metadata": {},
   "source": [
    "🔄 4. Schema Evolution\n",
    "But we can allow evolution like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a8522415-bad3-480b-a135-19153d9afb05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evolving = spark.createDataFrame([\n",
    "    (10001, \"New Guy\", 22, \"Fantasyland\", \"guy@delta.com\", \"extra info\")\n",
    "], [\"id\", \"name\", \"age\", \"country\", \"email\", \"note\"])\n",
    "\n",
    "evolving.write.format(\"delta\").mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(\"/data/schema-people-delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0560f7f2-3454-4c14-bd5e-17b9c105cbdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- note: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"/data/schema-people-delta\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0889a2f4-6273-4899-91f2-d86cb4da3150",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----+-----------+-----------------+----------+\n",
      "|   id|      name| age|    country|            email|      note|\n",
      "+-----+----------+----+-----------+-----------------+----------+\n",
      "|10001|   New Guy|  22|Fantasyland|    guy@delta.com|extra info|\n",
      "|99999|   New Guy|  22|Fantasyland|    guy@delta.com|extra info|\n",
      "|10001|Alice Test|  36|  Neverland|alice@updated.com|          |\n",
      "|10002|  New User|  40|    Wakanda|    new@delta.com|          |\n",
      "|99999|      John|null|       null|             null|      null|\n",
      "+-----+----------+----+-----------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"/data/schema-people-delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2136ae-73a2-4bbc-ba2d-36ab5760cbcc",
   "metadata": {},
   "source": [
    "🧬 5. Upserts (MERGE INTO)\n",
    "Let’s simulate an upsert (update existing or insert new):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a486721d-63b3-43e3-a197-848963df0a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- note: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.toDF().printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2b84d864-6d07-463e-9b02-f2f18fec6aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt = DeltaTable.forPath(spark, \"/data/schema-people-delta\")\n",
    "updates = spark.createDataFrame([\n",
    "    (10001, \"Alice Test\", 36, \"Neverland\", \"alice@updated.com\",\"\"),  # updated\n",
    "    (10002, \"New User\", 40, \"Wakanda\", \"new@delta.com\",\"\")           # inserted\n",
    "], [\"id\", \"name\", \"age\", \"country\", \"email\",\"note\"])\n",
    "\n",
    "dt.alias(\"target\").merge(\n",
    "    updates.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ef62ad4e-ec06-4532-8714-64e82b632f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----+-----------+-----------------+----------+\n",
      "|   id|      name| age|    country|            email|      note|\n",
      "+-----+----------+----+-----------+-----------------+----------+\n",
      "|10001|Alice Test|  36|  Neverland|alice@updated.com|          |\n",
      "|10001|Alice Test|  36|  Neverland|alice@updated.com|          |\n",
      "|10002|  New User|  40|    Wakanda|    new@delta.com|          |\n",
      "|99999|   New Guy|  22|Fantasyland|    guy@delta.com|extra info|\n",
      "|99999|      John|null|       null|             null|      null|\n",
      "+-----+----------+----+-----------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af933da8-a2f9-4c87-b6a8-1c6b2b089c98",
   "metadata": {},
   "source": [
    "🔥 6. Delete & Update\n",
    "Delta supports SQL-like updates and deletes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "70d3155e-93df-459e-a31a-7781680d8860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete people older than 75\n",
    "dt.delete(\"age > 35\")\n",
    "\n",
    "# Update country name\n",
    "dt.update(\"country = 'Neverland'\", {\"country\": \"'Fantasy Realm'\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73de1ccf-5cdd-47ed-87aa-47fa1a5bc4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+-----------+-------------+----------+\n",
      "|   id|   name| age|    country|        email|      note|\n",
      "+-----+-------+----+-----------+-------------+----------+\n",
      "|99999|New Guy|  22|Fantasyland|guy@delta.com|extra info|\n",
      "|99999|   John|null|       null|         null|      null|\n",
      "+-----+-------+----+-----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4485bb3d-c921-415c-bcbe-bcc40593a98f",
   "metadata": {
    "tags": []
   },
   "source": [
    "📦 7. File Compaction (Optimize)\n",
    "Delta creates many small parquet files. You can compact them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "249d2a28-b512-46a4-9422-c974e79b6745",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".part-00000-2590be05-8f9a-4def-b16c-fc3a3ceb07fa-c000.snappy.parquet.crc\n",
      ".part-00000-94e50324-6549-441a-8071-ab3f0e5a2206-c000.snappy.parquet.crc\n",
      ".part-00000-b054e596-bf15-40a1-b781-c57c2259e45a-c000.snappy.parquet.crc\n",
      ".part-00000-b7a2b6e5-457c-4e04-ad8f-3c62716c44e1-c000.snappy.parquet.crc\n",
      ".part-00000-d69dc4c4-ee68-4bda-875c-fa6ea055c6a0-c000.snappy.parquet.crc\n",
      ".part-00000-f5dbb791-68ea-42bb-b2ef-ca39e93e88ca-c000.snappy.parquet.crc\n",
      ".part-00011-360c616d-c8c1-40cd-94cd-a825448dbd34-c000.snappy.parquet.crc\n",
      ".part-00011-4c5462e2-a77b-4fbc-819e-8fd446f25b5b-c000.snappy.parquet.crc\n",
      ".part-00011-7f789d97-502f-47f0-b469-a7807898c4f1-c000.snappy.parquet.crc\n",
      "_delta_log\n",
      "part-00000-2590be05-8f9a-4def-b16c-fc3a3ceb07fa-c000.snappy.parquet\n",
      "part-00000-94e50324-6549-441a-8071-ab3f0e5a2206-c000.snappy.parquet\n",
      "part-00000-b054e596-bf15-40a1-b781-c57c2259e45a-c000.snappy.parquet\n",
      "part-00000-b7a2b6e5-457c-4e04-ad8f-3c62716c44e1-c000.snappy.parquet\n",
      "part-00000-d69dc4c4-ee68-4bda-875c-fa6ea055c6a0-c000.snappy.parquet\n",
      "part-00000-f5dbb791-68ea-42bb-b2ef-ca39e93e88ca-c000.snappy.parquet\n",
      "part-00011-360c616d-c8c1-40cd-94cd-a825448dbd34-c000.snappy.parquet\n",
      "part-00011-4c5462e2-a77b-4fbc-819e-8fd446f25b5b-c000.snappy.parquet\n",
      "part-00011-7f789d97-502f-47f0-b469-a7807898c4f1-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Inspect parquet files\n",
    "import os\n",
    "\n",
    "log_files = sorted(os.listdir(f\"/data/schema-people-delta\"))\n",
    "for f in log_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7c956f7-1ec3-4f30-9de9-3e79f797d417",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeltaTable.forPath(spark, \"/data/schema-people-delta\").optimize().executeCompaction()  # If you're using Delta OSS with support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "81e450cb-35de-4e0b-94fe-bb7faafa009f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".part-00000-2590be05-8f9a-4def-b16c-fc3a3ceb07fa-c000.snappy.parquet.crc - 0.02 KB\n",
      ".part-00000-75808793-61b2-4ea7-b9dc-214c22c54823-c000.snappy.parquet.crc - 0.02 KB\n",
      ".part-00000-94e50324-6549-441a-8071-ab3f0e5a2206-c000.snappy.parquet.crc - 0.02 KB\n",
      ".part-00000-b054e596-bf15-40a1-b781-c57c2259e45a-c000.snappy.parquet.crc - 0.02 KB\n",
      ".part-00000-b7a2b6e5-457c-4e04-ad8f-3c62716c44e1-c000.snappy.parquet.crc - 0.02 KB\n",
      ".part-00000-d69dc4c4-ee68-4bda-875c-fa6ea055c6a0-c000.snappy.parquet.crc - 0.01 KB\n",
      ".part-00000-f5dbb791-68ea-42bb-b2ef-ca39e93e88ca-c000.snappy.parquet.crc - 0.02 KB\n",
      ".part-00011-360c616d-c8c1-40cd-94cd-a825448dbd34-c000.snappy.parquet.crc - 0.02 KB\n",
      ".part-00011-4c5462e2-a77b-4fbc-819e-8fd446f25b5b-c000.snappy.parquet.crc - 0.02 KB\n",
      ".part-00011-7f789d97-502f-47f0-b469-a7807898c4f1-c000.snappy.parquet.crc - 0.02 KB\n",
      "part-00000-2590be05-8f9a-4def-b16c-fc3a3ceb07fa-c000.snappy.parquet - 1.76 KB\n",
      "part-00000-75808793-61b2-4ea7-b9dc-214c22c54823-c000.snappy.parquet - 1.82 KB\n",
      "part-00000-94e50324-6549-441a-8071-ab3f0e5a2206-c000.snappy.parquet - 1.93 KB\n",
      "part-00000-b054e596-bf15-40a1-b781-c57c2259e45a-c000.snappy.parquet - 0.69 KB\n",
      "part-00000-b7a2b6e5-457c-4e04-ad8f-3c62716c44e1-c000.snappy.parquet - 0.69 KB\n",
      "part-00000-d69dc4c4-ee68-4bda-875c-fa6ea055c6a0-c000.snappy.parquet - 0.37 KB\n",
      "part-00000-f5dbb791-68ea-42bb-b2ef-ca39e93e88ca-c000.snappy.parquet - 0.69 KB\n",
      "part-00011-360c616d-c8c1-40cd-94cd-a825448dbd34-c000.snappy.parquet - 0.7 KB\n",
      "part-00011-4c5462e2-a77b-4fbc-819e-8fd446f25b5b-c000.snappy.parquet - 1.8 KB\n",
      "part-00011-7f789d97-502f-47f0-b469-a7807898c4f1-c000.snappy.parquet - 1.8 KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"/data/schema-people-delta\"\n",
    "log_files = sorted(os.listdir(path))\n",
    "\n",
    "for f in log_files:\n",
    "    full_path = os.path.join(path, f)\n",
    "    if os.path.isfile(full_path):\n",
    "        size_bytes = os.path.getsize(full_path)\n",
    "        size_kb = round(size_bytes / 1024, 2)\n",
    "        print(f\"{f} - {size_kb} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc630b-a395-4e65-8a20-187b627b5bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
