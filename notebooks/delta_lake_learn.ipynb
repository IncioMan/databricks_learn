{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207ca7b7-b942-48ca-bffc-9e264ed2cfee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  3|Cathy|\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeDemo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sample DataFrame\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Cathy\")\n",
    "], [\"id\", \"name\"])\n",
    "\n",
    "# Write as Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/data/people\")\n",
    "\n",
    "# Read Delta table\n",
    "df2 = spark.read.format(\"delta\").load(\"/data/people\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3b167-4ec4-4fec-9e1d-3e0a2961a340",
   "metadata": {},
   "source": [
    "# 🔁 1. ACID Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd2f75-6aed-41e0-9e02-0b31a2a9c388",
   "metadata": {
    "tags": []
   },
   "source": [
    "✅ **What Are ACID Transactions?**  \n",
    "ACID stands for:\n",
    "\n",
    "- **Atomicity** → each write is all-or-nothing  \n",
    "- **Consistency** → data moves from one valid state to another  \n",
    "- **Isolation** → concurrent writes don’t interfere  \n",
    "- **Durability** → once a write is committed, it's permanent  \n",
    "\n",
    "Delta Lake brings ACID to data lakes by using a transaction log — just like a database does — even though the data is stored in flat files (Parquet) in object storage or local disk.\n",
    "\n",
    "---\n",
    "\n",
    "🔧 **How Delta Achieves ACID Transactions**  \n",
    "Let’s break it down into mechanisms.\n",
    "\n",
    "### 1. Transaction Log (`_delta_log/`)\n",
    "Every write operation (insert, delete, update, merge, etc.) creates a new log file like:\n",
    "\n",
    "```logs\n",
    "/data/people-delta/_delta_log/00000000000000000000.json\n",
    "/data/people-delta/_delta_log/00000000000000000001.json\n",
    "```\n",
    "\n",
    "\n",
    "Each log file contains:\n",
    "- **Metadata** (`metaData`)  \n",
    "- **Operation info** (`commitInfo`)  \n",
    "- **A list of files added or removed** (`add`, `remove` entries)  \n",
    "\n",
    "🧠 The data itself is immutable Parquet files. Delta just manages which files are “active” by listing them in the latest log version.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Atomicity\n",
    "When you write data:\n",
    "\n",
    "- Delta writes the new Parquet files first.  \n",
    "- Only if *all* new files are successfully written, it appends a JSON log file with `add` entries.  \n",
    "- If anything fails, nothing gets committed to the `_delta_log`, and the table state remains unchanged.  \n",
    "\n",
    "This is atomic: **either the whole change is visible, or none of it is.**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Isolation via File Locking\n",
    "Delta uses concurrency control:\n",
    "\n",
    "- **Local file systems:** it can use file locks  \n",
    "- **Cloud stores like S3:** it uses a protocol called *Optimistic Concurrency Control (OCC)*  \n",
    "\n",
    "With OCC:\n",
    "- A transaction reads the current state (e.g., version 12)  \n",
    "- It prepares new changes based on that state  \n",
    "- When it commits, it checks whether version 12 is still the latest  \n",
    "- If someone else has committed version 13 in the meantime → it fails and retries  \n",
    "\n",
    "This keeps multiple concurrent writers isolated and avoids conflicts.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Durability\n",
    "The `_delta_log` is an append-only log, and once written:\n",
    "\n",
    "- It is **never modified**  \n",
    "- It is considered the **source of truth**  \n",
    "- Even if the cluster crashes, you can rebuild the table’s current state from these logs  \n",
    "\n",
    "That’s your **durability guarantee**.\n",
    "\n",
    "---\n",
    "\n",
    "⚠️ **Does It Affect Performance?**  \n",
    "Yes, but in a good way (mostly)!\n",
    "\n",
    "✅ **Benefits**\n",
    "- Querying is faster: because the log tells you exactly which files to read, instead of scanning the whole folder  \n",
    "- Appends and updates are efficient and safe  \n",
    "\n",
    "⚠️ **Trade-offs**\n",
    "- Small writes result in many small Parquet files → can affect read performance  \n",
    "- That’s why Delta has **Optimize** and **Z-Ordering** to compact and organize files  \n",
    "- For large-scale use, write performance is *slightly* slower than blind dumping to Parquet — but you gain **huge advantages** in reliability, auditability, and maintainability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7d7eb4e-502a-4bd2-8a3c-9f5218be6c37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000000.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": false,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "4",
         "numOutputBytes": "2522",
         "numOutputRows": "3"
        },
        "operationParameters": {
         "mode": "Overwrite",
         "partitionBy": "[]"
        },
        "timestamp": 1745399994525,
        "txnId": "5524117a-1ddb-4c22-9145-9825949f65ed"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "protocol": {
        "minReaderVersion": 1,
        "minWriterVersion": 2
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "metaData": {
        "configuration": {},
        "createdTime": 1745399994116,
        "format": {
         "options": {},
         "provider": "parquet"
        },
        "id": "81c269bc-aa0c-4607-ba98-e1bedf7cbbc6",
        "partitionColumns": [],
        "schemaString": "{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399994411,
        "partitionValues": {},
        "path": "part-00003-567c0df7-46a4-40ad-8c72-df42d46acc0b-c000.snappy.parquet",
        "size": 719,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":1,\"name\":\"Alice\"},\"maxValues\":{\"id\":1,\"name\":\"Alice\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399994426,
        "partitionValues": {},
        "path": "part-00007-6f389545-6fd8-48c0-8dcb-8ad922c1955c-c000.snappy.parquet",
        "size": 705,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":2,\"name\":\"Bob\"},\"maxValues\":{\"id\":2,\"name\":\"Bob\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399994422,
        "partitionValues": {},
        "path": "part-00011-d731241f-f722-408e-839e-22e519881bad-c000.snappy.parquet",
        "size": 719,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":3,\"name\":\"Cathy\"},\"maxValues\":{\"id\":3,\"name\":\"Cathy\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000001.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "1091",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 0,
        "timestamp": 1745399996132,
        "txnId": "ea5b6776-57e9-4aee-8fbe-2486a4208079"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399996108,
        "partitionValues": {},
        "path": "part-00011-117cd441-0a9e-4987-adfa-f23306713746-c000.snappy.parquet",
        "size": 712,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":1,\"name\":\"test\"},\"maxValues\":{\"id\":1,\"name\":\"test\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Path to your Delta table\n",
    "path = \"/data/test-acid\"\n",
    "\n",
    "# Delete the entire folder\n",
    "shutil.rmtree(path, ignore_errors=True)\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Cathy\")\n",
    "], [\"id\", \"name\"])\n",
    "\n",
    "# Write some data\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/data/test-acid\")\n",
    "\n",
    "# Append new row\n",
    "spark.createDataFrame([(4, \"test\")], [\"id\", \"name\"]) \\\n",
    "    .write.format(\"delta\").mode(\"append\").save(\"/data/test-acid\")\n",
    "\n",
    "from IPython.display import display, JSON\n",
    "\n",
    "log_dir = \"/data/test-acid/_delta_log\"\n",
    "log_files = sorted([f for f in os.listdir(log_dir) if f.endswith(\".json\")])\n",
    "\n",
    "for file in log_files:\n",
    "    print(f\"\\n>>> Log version: {file}\")\n",
    "    with open(os.path.join(log_dir, file)) as f:\n",
    "        for line in f.readlines():\n",
    "            parsed = json.loads(line)\n",
    "            display(JSON(parsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f936e9f-7ad1-4da7-a9c0-868745410b1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## _delta_logs Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653083c9-8ac0-4774-96dc-20bb6a9eb01b",
   "metadata": {},
   "source": [
    "### 🧾 `commitInfo` Example Breakdown\n",
    "\n",
    "This is the metadata for the commit.\n",
    "\n",
    "- **`timestamp`**:  \n",
    "  The time when the operation was committed (in milliseconds since Unix epoch).\n",
    "\n",
    "- **`operation`**:  \n",
    "  This log entry represents a `WRITE` operation (it could also be `INSERT`, `UPDATE`, `DELETE`, etc.).  \n",
    "  Here, it’s a `WRITE` operation.\n",
    "\n",
    "- **`operationParameters`**:\n",
    "  - **`mode`**: `Overwrite` — the data is being written to the table, replacing the previous content.  \n",
    "  - **`partitionBy`**: `[]` — indicates the partitioning scheme. An empty list means the table isn’t partitioned.  \n",
    "  - **`isolationLevel`**: `Serializable` — the highest level of isolation (no other operations can interfere during this transaction).  \n",
    "  - **`isBlindAppend`**: `False` — this is not a simple append; it’s a full overwrite.\n",
    "\n",
    "- **`operationMetrics`**:  \n",
    "  Provides stats for the operation:\n",
    "  - **`numFiles`**: 4 new files were written.  \n",
    "  - **`numOutputRows`**: 3 rows of data were written.  \n",
    "  - **`numOutputBytes`**: 2522 bytes — the total size of the written data.\n",
    "\n",
    "- **`txnId`**:  \n",
    "  The transaction ID (unique to the transaction).  \n",
    "  Used to track this specific operation in Delta’s transaction log.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f78bc17-9518-43a9-9394-3fce2f18f027",
   "metadata": {},
   "source": [
    "### 📦 `metaData` Example Breakdown\n",
    "\n",
    "Contains schema and metadata information for the Delta table.\n",
    "\n",
    "- **`id`**:  \n",
    "  A unique ID for the Delta table (used internally for reference).\n",
    "\n",
    "- **`format`**:  \n",
    "  The format used for storing data.  \n",
    "  Here, it's `parquet`, the underlying storage format for Delta.\n",
    "\n",
    "- **`schemaString`**:  \n",
    "  The schema of the table, defined as a JSON string. This tells you the structure of the data (columns, types, etc.).  \n",
    "  In this case:\n",
    "  - `id`: A `long` field (nullable)  \n",
    "  - `name`: A `string` field (nullable)\n",
    "\n",
    "- **`partitionColumns`**:  \n",
    "  The table isn’t partitioned (`[]`).\n",
    "\n",
    "- **`configuration`**:  \n",
    "  Additional configuration (empty here).\n",
    "\n",
    "- **`createdTime`**:  \n",
    "  The time the table was created (in milliseconds since Unix epoch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590804e6-7934-4f8e-ae9a-54fb62d17619",
   "metadata": {},
   "source": [
    "### ➕ `add` Entry Breakdown\n",
    "\n",
    "This entry indicates that a new Parquet file was added to the Delta table.\n",
    "\n",
    "- **`path`**:  \n",
    "  The path to the newly added Parquet file.\n",
    "\n",
    "- **`partitionValues`**:  \n",
    "  Empty here since the table isn’t partitioned.\n",
    "\n",
    "- **`size`**:  \n",
    "  The size of the Parquet file in bytes (`719` bytes).\n",
    "\n",
    "- **`modificationTime`**:  \n",
    "  The timestamp when the file was last modified.\n",
    "\n",
    "- **`dataChange`**:  \n",
    "  `True` means that this file contains **data changes** (not just metadata).\n",
    "\n",
    "- **`stats`**:  \n",
    "  The statistics for the data in this file:\n",
    "  - **`numRecords`**: This file contains 1 record.  \n",
    "  - **`minValues`** and **`maxValues`**:  \n",
    "    The minimum and maximum values of the fields in the file.  \n",
    "    Helps optimize queries (e.g., for pruning files based on min/max).  \n",
    "  - **`nullCount`**:  \n",
    "    The number of null values in each field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eab660-97a3-4725-be08-d037254a6b48",
   "metadata": {},
   "source": [
    "# ⏳ Time Travel in Delta Lake\n",
    "\n",
    "Time Travel in Delta Lake allows you to query historical versions of your data, even after updates, deletes, or appends. This feature is possible thanks to Delta Lake’s transaction log.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 How Time Travel Works\n",
    "\n",
    "Delta Lake uses the `_delta_log` to track all changes to the table. Each write operation (INSERT, UPDATE, DELETE, etc.) appends a new JSON file to the log, containing metadata about the operation.\n",
    "\n",
    "#### 🗂 Transaction Log\n",
    "Each commit is stored as a new JSON file in `_delta_log/`, including:\n",
    "- The operation type (`WRITE`, `UPDATE`, `DELETE`)\n",
    "- The schema at that point\n",
    "- Data files written or removed\n",
    "- The resulting table version\n",
    "\n",
    "#### 🔢 Versions\n",
    "Each commit increments the table version:\n",
    "- Version 0 is the initial state\n",
    "- Version 1 is the next, and so on\n",
    "- Each commit includes a unique `txnId`\n",
    "\n",
    "#### 🧾 Snapshotting\n",
    "When querying, Delta reads the latest log version and constructs a snapshot.  \n",
    "To read an older version, it uses the corresponding log version and rebuilds the table as it was.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Mechanics Behind Time Travel\n",
    "\n",
    "- **Immutable Logs**: Once written, logs are never changed.\n",
    "- **Immutable Data**: Parquet files are also immutable.\n",
    "- **Versioning**: Delta tracks which files belong to each version.\n",
    "- **Garbage Collection**: Old versions are cleaned up via `VACUUM` after a retention period.\n",
    "\n",
    "---\n",
    "\n",
    "### 📅 How to Query Historical Versions\n",
    "\n",
    "#### By Version Number\n",
    "```python\n",
    "df = spark.read.format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 0) \\\n",
    "    .load(\"path_to_table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7b62d-eafc-4531-962f-972cccd75310",
   "metadata": {},
   "source": [
    "#### 🕰️ By Timestamp\n",
    "Delta finds the most recent version that was committed **before** the specified timestamp and returns the snapshot of that version.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Performance Considerations of Time Travel\n",
    "\n",
    "#### 🔄 **Read Performance**:\n",
    "Time travel operations are efficient because Delta doesn’t re-read all data. Instead, it:\n",
    "- Uses the transaction log and metadata to only read necessary files.\n",
    "- Might need to read multiple files, but avoids scanning the entire table.\n",
    "- Leverages Delta’s query optimizer, which efficiently prunes data based on the log's statistics.\n",
    "\n",
    "#### 🗑️ **Garbage Collection**:\n",
    "- Time travel doesn't indefinitely increase storage size.\n",
    "- Delta performs garbage collection to remove old versions when they are no longer needed.\n",
    "- This process is controlled by the retention period and can be manually triggered using the `VACUUM` command to delete old data files no longer referenced.\n",
    "\n",
    "#### 💾 **Storage Overhead**:\n",
    "- There is some overhead due to new Parquet files and logs being created with each version.\n",
    "- However, this is minimized by Delta’s efficient handling of small incremental writes.\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 Example Use Case\n",
    "\n",
    "Suppose you want to go back to a version of the Delta table as it was on **2025-04-20 at 15:30**. Here’s how you would proceed:\n",
    "\n",
    "1. **Check Table History** to find the closest version:\n",
    "```python\n",
    "delta_table = DeltaTable.forPath(spark, \"path_to_table\")\n",
    "history_df = delta_table.history()\n",
    "history_df.show(truncate=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0bee0f1-17d4-4c53-ba06-283a35444e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|1      |2025-04-23 09:19:56.172|null  |null    |WRITE    |{mode -> Append, partitionBy -> []}   |null|null    |null     |0          |Serializable  |true         |{numFiles -> 2, numOutputRows -> 1, numOutputBytes -> 1091}|null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "|0      |2025-04-23 09:19:54.588|null  |null    |WRITE    |{mode -> Overwrite, partitionBy -> []}|null|null    |null     |null       |Serializable  |false        |{numFiles -> 4, numOutputRows -> 3, numOutputBytes -> 2522}|null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"/data/test-acid\")\n",
    "history_df = delta_table.history()\n",
    "history_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a771109-e399-4faa-9448-e684367502f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_table.delete(\"id = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b255363-e631-4683-a70b-3d4f5aa0fce6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                           |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|2      |2025-04-23 09:33:20.121|null  |null    |DELETE   |{predicate -> [\"(id = 1L)\"]}          |null|null    |null     |1          |Serializable  |false        |{numRemovedFiles -> 2, numRemovedBytes -> 1431, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 964, numDeletedRows -> 2, scanTimeMs -> 654, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 310}|null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "|1      |2025-04-23 09:19:56.172|null  |null    |WRITE    |{mode -> Append, partitionBy -> []}   |null|null    |null     |0          |Serializable  |true         |{numFiles -> 2, numOutputRows -> 1, numOutputBytes -> 1091}                                                                                                                                                                |null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "|0      |2025-04-23 09:19:54.588|null  |null    |WRITE    |{mode -> Overwrite, partitionBy -> []}|null|null    |null     |null       |Serializable  |false        |{numFiles -> 4, numOutputRows -> 3, numOutputBytes -> 2522}                                                                                                                                                                |null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_df = delta_table.history()\n",
    "history_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c038e626-dc6f-4b08-a2a2-539b7f7e254e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  1| test|\n",
      "+---+-----+\n",
      "\n",
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/data/test-acid\").filter(\"id = 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b0cb866-d670-4579-91b5-a13b7d81246f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"/data/test-acid\").filter(\"id = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c36ac3f-c3be-4a4b-9255-b5f0fbf9c8e3",
   "metadata": {},
   "source": [
    "How Deletes Are Handled in Delta Lake\n",
    "Delta Lake is append-only under the hood. It never modifies existing data files directly. Instead, it follows a copy-on-write strategy:\n",
    "\n",
    "1. Query Optimization and File Pruning\n",
    "When you run a DELETE or an UPDATE, Delta first identifies which Parquet files contain the rows to be deleted using the statistics in the _delta_log (like minValues, maxValues, and nullCounts).\n",
    "\n",
    "Only those files are read and rewritten. Others are untouched.\n",
    "\n",
    "2. Rewrite with Changes\n",
    "Delta reads the affected Parquet files, applies the delete condition (e.g., WHERE id = 42), and writes new files with the remaining (non-deleted) records.\n",
    "\n",
    "The old Parquet files are logically removed — they are not physically deleted immediately but are marked as removed in the _delta_log.\n",
    "\n",
    "3. Transaction Log Update\n",
    "The _delta_log records:\n",
    "\n",
    "A new version with a commitInfo block describing the DELETE operation.\n",
    "\n",
    "A set of remove actions, each pointing to a deleted (now obsolete) data file.\n",
    "\n",
    "A set of add actions for the newly written files.\n",
    "\n",
    "So from a transactional point of view:\n",
    "\n",
    "The table instantly reflects the updated state (with the records deleted).\n",
    "\n",
    "Old files still exist and can be used to reconstruct previous versions — which is what enables time travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04f8ee1b-2a8b-406b-8032-ec5787ff2b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000000.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": false,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "4",
         "numOutputBytes": "2522",
         "numOutputRows": "3"
        },
        "operationParameters": {
         "mode": "Overwrite",
         "partitionBy": "[]"
        },
        "timestamp": 1745399994525,
        "txnId": "5524117a-1ddb-4c22-9145-9825949f65ed"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "protocol": {
        "minReaderVersion": 1,
        "minWriterVersion": 2
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "metaData": {
        "configuration": {},
        "createdTime": 1745399994116,
        "format": {
         "options": {},
         "provider": "parquet"
        },
        "id": "81c269bc-aa0c-4607-ba98-e1bedf7cbbc6",
        "partitionColumns": [],
        "schemaString": "{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399994411,
        "partitionValues": {},
        "path": "part-00003-567c0df7-46a4-40ad-8c72-df42d46acc0b-c000.snappy.parquet",
        "size": 719,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":1,\"name\":\"Alice\"},\"maxValues\":{\"id\":1,\"name\":\"Alice\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399994426,
        "partitionValues": {},
        "path": "part-00007-6f389545-6fd8-48c0-8dcb-8ad922c1955c-c000.snappy.parquet",
        "size": 705,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":2,\"name\":\"Bob\"},\"maxValues\":{\"id\":2,\"name\":\"Bob\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399994422,
        "partitionValues": {},
        "path": "part-00011-d731241f-f722-408e-839e-22e519881bad-c000.snappy.parquet",
        "size": 719,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":3,\"name\":\"Cathy\"},\"maxValues\":{\"id\":3,\"name\":\"Cathy\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000001.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": true,
        "isolationLevel": "Serializable",
        "operation": "WRITE",
        "operationMetrics": {
         "numFiles": "2",
         "numOutputBytes": "1091",
         "numOutputRows": "1"
        },
        "operationParameters": {
         "mode": "Append",
         "partitionBy": "[]"
        },
        "readVersion": 0,
        "timestamp": 1745399996132,
        "txnId": "ea5b6776-57e9-4aee-8fbe-2486a4208079"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "add": {
        "dataChange": true,
        "modificationTime": 1745399996108,
        "partitionValues": {},
        "path": "part-00011-117cd441-0a9e-4987-adfa-f23306713746-c000.snappy.parquet",
        "size": 712,
        "stats": "{\"numRecords\":1,\"minValues\":{\"id\":1,\"name\":\"test\"},\"maxValues\":{\"id\":1,\"name\":\"test\"},\"nullCount\":{\"id\":0,\"name\":0}}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Log version: 00000000000000000002.json\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "commitInfo": {
        "engineInfo": "Apache-Spark/3.3.2 Delta-Lake/2.3.0",
        "isBlindAppend": false,
        "isolationLevel": "Serializable",
        "operation": "DELETE",
        "operationMetrics": {
         "executionTimeMs": "964",
         "numAddedBytes": "0",
         "numAddedChangeFiles": "0",
         "numAddedFiles": "0",
         "numCopiedRows": "0",
         "numDeletedRows": "2",
         "numRemovedBytes": "1431",
         "numRemovedFiles": "2",
         "rewriteTimeMs": "310",
         "scanTimeMs": "654"
        },
        "operationParameters": {
         "predicate": "[\"(id = 1L)\"]"
        },
        "readVersion": 1,
        "timestamp": 1745400800049,
        "txnId": "2984792e-f799-44db-a9f2-2238fb5c88b1"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "remove": {
        "dataChange": true,
        "deletionTimestamp": 1745400800049,
        "extendedFileMetadata": true,
        "partitionValues": {},
        "path": "part-00003-567c0df7-46a4-40ad-8c72-df42d46acc0b-c000.snappy.parquet",
        "size": 719
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "remove": {
        "dataChange": true,
        "deletionTimestamp": 1745400800049,
        "extendedFileMetadata": true,
        "partitionValues": {},
        "path": "part-00011-117cd441-0a9e-4987-adfa-f23306713746-c000.snappy.parquet",
        "size": 712
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, JSON\n",
    "\n",
    "log_dir = \"/data/test-acid/_delta_log\"\n",
    "log_files = sorted([f for f in os.listdir(log_dir) if f.endswith(\".json\")])\n",
    "\n",
    "for file in log_files:\n",
    "    print(f\"\\n>>> Log version: {file}\")\n",
    "    with open(os.path.join(log_dir, file)) as f:\n",
    "        for line in f.readlines():\n",
    "            parsed = json.loads(line)\n",
    "            display(JSON(parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf67aa-8cd2-4aba-82eb-099d0d02e835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
